:mod:`netpro2vec.Netpro2vec`
============================

.. py:module:: netpro2vec.Netpro2vec


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   netpro2vec.Netpro2vec.Netpro2vec



.. py:class:: Netpro2vec(format='graphml', dimensions=128, prob_type: List[str] = ['tm1'], extractor=[1], cut_off=[0.01], agg_by=[5], min_count=5, down_sampling=0.0001, workers=4, epochs=10, learning_rate=0.025, remove_inf=False, vertex_labels=False, seed=0, verbose=False)

   The class implementation of "NETPRO2VEC" model for whole-graph embedding.
   from the IEEE TCBB '20 paper "Netpro2vec: a Graph Embedding Framework for Biological Networks". The procedure
   uses probability distribution representations of graphs and skip-gram learning modelor later.

   Args:
       **format** *(str, optional)* -  graph format. Dfault is "graphml"

       **dimensions** *(int, optional)* – number of features. Default is 128.

       **rob_type** *(list of str, optional)* –  list of probability types. Default is ["tm1"] (allowed values: "ndd", "tm<int>").

       **cut_off** *(list of float, optional)* –  list of cut-off thresholds to form words. Default is [0.01].

       **agg_by** *(list of int, optional* – list of numbers of aggregators in words. Default is [0].

       **walk** *(int, optional)* –  number of random walks in TM calculation. Default is 1.

       **min_count** *(int, optional)* – Ignores all words with total frequency lower than this (Doc2Vec). Default is 5

       **workers** *(int, optional)* – use these many worker threads to train the model (Doc2Vec). Default is 4

       **epochs** *(int, optional)* – Number of iterations (epochs) over the corpus (Doc2Vec). Default is 10

       **remove_inf** *(bool, optional)*: flag for removal of infinity value in histogram bins. Default is False.

       **vertex_labels** *(bool, optional)*: flag to set if graphs have vertex labels to be considered. Default is False

   .. method:: fit(self, graphs: List[ig.Graph])

      Fitting method of Netpro2vec model.

      Args:
          **graphs** *(List igraph.Graph objs)* - list of graphs in igraph format types.

      Return:
                  The trained **Netpro2vec** model.


   .. method:: get_embedding(self)

      Access embedding of Netpro2vec model.

      Return:
                  The produced embedding in numpy array format (None if the mode was not trained).


   .. method:: get_memberships(self)

      Access last document list used for training the Netpro2vec model.

      Return:
                  The document list used in last model training ([] if the model was never trained).


   .. method:: __generate_probabilities(self, graphs: List[ig.Graph])


   .. method:: __batch_feature_extractor(self, probability_distrib_matrix, name, word_tag=None, tag=True, aggregate=0, cut=0, encodew=True, extractor=1)

      Generate a document collection describing a single graph
      by concatenating distribution matrix data 
      (NDD or TM<int>) for each node in the graph. 


   .. method:: __get_document_collections(self, workers=4, tag_doc=True, encodew=True)

      Generate documents for graphs. 
      If multiple distributions are specified, documents are generated for each
      distribution, then merged ito una single vocabulary.


   .. method:: __get_diction_corpus(self)


   .. method:: __run_d2v(self, dimensions=128, min_count=5, down_sampling=0.0001, workers=4, epochs=10, learning_rate=0.025)

      Run Doc2Vec library to:
      1) produce the vocabulary from the list of documents representing the graphs
      2) train the model on the vocabulary.
      3) produce the embedding matrix, then store in the "embedding" attribute of the 
      Netpro2vec object.



